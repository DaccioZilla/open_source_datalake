# docker_spark_airflow
Tudo o que foi necessáro para fazer Orquestração de Spark com Airflow, tudo rodando me Docker

O projeto foi construído:

- Com uam imagem modificada do Airflow Docker (https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)
- Para a parte do Spark, MinIO e Delta Lake, deixo os créditos para o autor desse post no medium: https://medium.com/@ongxuanhong/dataops-02-spawn-up-apache-spark-infrastructure-by-using-docker-fec518698993. 
- Eu fiz algumas modificações para ter menos dockerfiles e arquivos de configuração do spark para gerenciar.
- Minha principal contribuição nisso doi um Dockerfile que expande a imagem do docker, instalando o Spark e atualizando o Python, para que o Airflow seja bem sucedido com o SparkSubimitOperator

Mais detalhes sobre esse projeto, talvez, quem sabe, se tornem um post no Medium.